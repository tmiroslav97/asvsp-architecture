[2024-11-05T21:39:05.612+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: initial_transform.hive_server_task manual__2024-11-05T21:39:04.419552+00:00 [queued]>
[2024-11-05T21:39:05.620+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: initial_transform.hive_server_task manual__2024-11-05T21:39:04.419552+00:00 [queued]>
[2024-11-05T21:39:05.621+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-11-05T21:39:05.635+0000] {taskinstance.py:1382} INFO - Executing <Task(SSHOperator): hive_server_task> on 2024-11-05 21:39:04.419552+00:00
[2024-11-05T21:39:05.645+0000] {standard_task_runner.py:57} INFO - Started process 1290 to run task
[2024-11-05T21:39:05.649+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'initial_transform', 'hive_server_task', 'manual__2024-11-05T21:39:04.419552+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/initial_transform.py', '--cfg-path', '/tmp/tmp3qu6kwin']
[2024-11-05T21:39:05.650+0000] {standard_task_runner.py:85} INFO - Job 48: Subtask hive_server_task
[2024-11-05T21:39:05.708+0000] {task_command.py:416} INFO - Running <TaskInstance: initial_transform.hive_server_task manual__2024-11-05T21:39:04.419552+00:00 [running]> on host c5156f201d33
[2024-11-05T21:39:05.812+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='initial_transform' AIRFLOW_CTX_TASK_ID='hive_server_task' AIRFLOW_CTX_EXECUTION_DATE='2024-11-05T21:39:04.419552+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-05T21:39:04.419552+00:00'
[2024-11-05T21:39:05.813+0000] {ssh.py:152} INFO - Creating ssh_client
[2024-11-05T21:39:05.813+0000] {ssh.py:300} WARNING - No Host Key Verification. This won't protect against Man-In-The-Middle attacks
[2024-11-05T21:39:05.823+0000] {transport.py:1893} INFO - Connected (version 2.0, client OpenSSH_7.4p1)
[2024-11-05T21:39:05.882+0000] {transport.py:1893} INFO - Authentication (password) successful!
[2024-11-05T21:39:05.883+0000] {ssh.py:478} INFO - Running command: /opt/hive/bin/beeline -u jdbc:hive2://localhost:10000 -e 'CREATE TABLE pokes2 (foo INT, bar STRING);'
[2024-11-05T21:39:08.777+0000] {ssh.py:529} WARNING - SLF4J: Class path contains multiple SLF4J bindings.
[2024-11-05T21:39:08.778+0000] {ssh.py:529} WARNING - SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[2024-11-05T21:39:08.778+0000] {ssh.py:529} WARNING - SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
[2024-11-05T21:39:08.778+0000] {ssh.py:529} WARNING - SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
[2024-11-05T21:39:08.779+0000] {ssh.py:529} WARNING - SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
[2024-11-05T21:39:08.825+0000] {ssh.py:529} WARNING - Connecting to jdbc:hive2://localhost:10000
[2024-11-05T21:39:09.111+0000] {ssh.py:529} WARNING - Connected to: Apache Hive (version 2.3.2)
[2024-11-05T21:39:09.112+0000] {ssh.py:529} WARNING - Driver: Hive JDBC (version 2.3.2)
[2024-11-05T21:39:09.112+0000] {ssh.py:529} WARNING - Transaction isolation: TRANSACTION_REPEATABLE_READ
[2024-11-05T21:39:09.188+0000] {ssh.py:529} WARNING - Error: org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table pokes2 already exists)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.security.AccessController.doPrivileged(Native Method)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[2024-11-05T21:39:09.189+0000] {ssh.py:529} WARNING - 	at java.lang.Thread.run(Thread.java:748)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table pokes2 already exists)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1232)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:255)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	... 11 more
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - Caused by: AlreadyExistsException(message:Table pokes2 already exists)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42052)
[2024-11-05T21:39:09.190+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42038)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:41964)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1199)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1185)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2399)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:752)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:740)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at com.sun.proxy.$Proxy34.createTable(Unknown Source)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2024-11-05T21:39:09.191+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2330)
[2024-11-05T21:39:09.192+0000] {ssh.py:529} WARNING - 	at com.sun.proxy.$Proxy34.createTable(Unknown Source)
[2024-11-05T21:39:09.192+0000] {ssh.py:529} WARNING - 	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:852)
[2024-11-05T21:39:09.192+0000] {ssh.py:529} WARNING - 	... 22 more (state=08S01,code=1)
[2024-11-05T21:39:09.192+0000] {ssh.py:529} WARNING - Closing: 0: jdbc:hive2://localhost:10000
[2024-11-05T21:39:09.259+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/ssh/operators/ssh.py", line 191, in execute
    result = self.run_ssh_client_command(ssh_client, self.command, context=context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/ssh/operators/ssh.py", line 179, in run_ssh_client_command
    self.raise_for_status(exit_status, agg_stderr, context=context)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/ssh/operators/ssh.py", line 173, in raise_for_status
    raise AirflowException(f"SSH operator error: exit status = {exit_status}")
airflow.exceptions.AirflowException: SSH operator error: exit status = 1
[2024-11-05T21:39:09.262+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=initial_transform, task_id=hive_server_task, execution_date=20241105T213904, start_date=20241105T213905, end_date=20241105T213909
[2024-11-05T21:39:09.274+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 48 for task hive_server_task (SSH operator error: exit status = 1; 1290)
[2024-11-05T21:39:09.312+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-11-05T21:39:09.335+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
